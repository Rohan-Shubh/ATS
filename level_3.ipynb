{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import zipfile\n","import chardet\n","from PyPDF2 import PdfReader\n","import pandas as pd\n","import json\n","import re\n","from groq import Groq\n","import time\n","import random\n","from groq import RateLimitError\n","\n","# Function to extract text from a PDF file\n","def extract_text_from_pdf(pdf_path):\n","    with open(pdf_path, 'rb') as file:\n","        reader = PdfReader(file)\n","        text = \"\"\n","        for page in reader.pages:\n","            text += page.extract_text()\n","    return text\n","\n","# Function to extract PDFs from zipfile, convert them to text, and save as .txt files\n","def extract_and_save_text_from_zip(zip_file_path, extract_to, output_zip_path):\n","    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_to)\n","\n","    with zipfile.ZipFile(output_zip_path, 'w') as output_zip:\n","        for root, dirs, files in os.walk(extract_to):\n","            for file in files:\n","                if file.endswith(\".pdf\"):\n","                    pdf_path = os.path.join(root, file)\n","                    text = extract_text_from_pdf(pdf_path)\n","\n","                    text_filename = os.path.splitext(file)[0] + \".txt\"\n","                    text_filepath = os.path.join(extract_to, text_filename)\n","                    with open(text_filepath, 'w', encoding='utf-8') as text_file:\n","                        text_file.write(text)\n","\n","                    output_zip.write(text_filepath, arcname=text_filename)\n","\n","    print(f\"Text files saved and zipped as {output_zip_path}\")\n","\n","# Function to detect file encoding\n","def detect_encoding(file_path):\n","    with open(file_path, 'rb') as f:\n","        result = chardet.detect(f.read())\n","    return result['encoding']\n","\n","# Function for sentiment calculation using Groq API\n","def sentiment_calculator(text_path):\n","    with open(text_path, 'r', encoding='Windows-1252') as f:\n","        text = f.read()\n","\n","    client = Groq(api_key= your_api_key)\n","\n","    completion = client.chat.completions.create(\n","        model=\"llama-3.1-70b-versatile\",\n","        messages=[\n","            {\n","                \"role\": \"system\",\n","                \"content\": \"\"\"\n","                You are provided a piece of text that contains various claims, both normal and exaggerated. Your task is to:\n","                  1. Identify all the claims in the text.\n","                  2. For normal claims, assign a sentiment score between 0 and 1.\n","                  3. For exaggerated claims, assign a sentiment score between 0 and 1.\n","                  4. Calculate the total sum of sentiment scores for normal claims and exaggerated claims separately.\n","                  5. Output the result as an integer, calculated using the formula:\n","                    (Average of normal sentiment scores) - 0.1 * (Average of exaggerated sentiment scores)\n","\n","                  DO NOT Provide any intermediate steps in the response.\n","\n","                  The final output should ONLY be the result of this formula.\n","                  \"\"\"\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"\"\"The text is provided below:\\n {text}\n","\n","                DO NOT Provide any intermediate steps in the response.\n","\n","                The final output should ONLY be the result of the formula mentioned above.\n","                \"\"\"\n","            }\n","        ],\n","        temperature=0.5,\n","        max_tokens=4096,\n","        top_p=1,\n","        stream=False,\n","        stop=None,\n","    )\n","\n","    return completion.choices[0].message.content\n","            \n","\n","# Function to extract IDs from file paths\n","def extract_ids(file_path):\n","    recommendee_id = re.search(r'Recommendation_Letters_of_ID_(\\d+)', file_path).group(1)\n","    recommender_id = re.search(r'Recommendation_From_ID_(\\d+)', file_path).group(1)\n","    return recommendee_id, recommender_id\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Extract text from resumes\n","    zip_file_path = \"Final_Resumes.zip\"\n","    extract_to = \"Files/Final_Resumes_Text\"\n","    output_zip_path = \"Files/extracted_text_files.zip\"\n","    extract_and_save_text_from_zip(zip_file_path, extract_to, output_zip_path)\n","\n","    # Process recommendation letters\n","    recommendation_dir = 'Final_Recommendation_Letters'\n","    sentiment_scores = {}\n","\n","    for root, dirs, files in os.walk(recommendation_dir):\n","        for file in files[:10]:\n","            if file.endswith(\".txt\"):\n","                file_path = os.path.join(root, file)\n","                file_encoding = detect_encoding(file_path)\n","                sentiment_score = sentiment_calculator(file_path)\n","                recommendee_id, recommender_id = extract_ids(file_path)\n","                sentiment_scores[(recommendee_id, recommender_id)] = float(sentiment_score)\n","\n","    # Load and process skills data\n","    df_lor_skills = pd.read_csv('innov8/LOR_skills.csv')\n","    df_lor_skills[['Recommendee_ID', 'Recommender_ID']] = df_lor_skills['File'].apply(lambda x: pd.Series(extract_ids(x)))\n","    df_lor_skills = df_lor_skills.drop('File', axis=1)\n","    df_lor_skills = df_lor_skills.sort_values('Recommendee_ID')\n","\n","    # Load and process resume data\n","    with open('innov8/all_resumes_data.json') as f:\n","        resume_data = json.load(f)\n","\n","    df_resume = pd.DataFrame(resume_data)\n","    df_resume['Recommendee_ID'] = df_resume['File_Name'].apply(lambda x: re.search(r'Resume_of_ID_(\\d+)', x).group(1))\n","    df_resume = df_resume[['Skills', 'Recommendee_ID']]\n","    df_resume = df_resume.sort_values('Recommendee_ID')\n","\n","    # Merge sentiment scores with skills data\n","    df_lor_skills['Sentiment_Score'] = df_lor_skills.apply(lambda row: sentiment_scores.get((row['Recommendee_ID'], row['Recommender_ID']), 0), axis=1)\n","\n","    # Save processed dataframes\n","    df_lor_skills.to_csv('Files/processed_lor_skills.csv', index=False)\n","    df_resume.to_csv('Files/processed_resume_skills.csv', index=False)\n","\n","    print(\"Data preparation and initial processing completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.walk('Final_Recommendation_Letters')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import RobertaTokenizer, RobertaModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load RoBERTa model and tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","model = RobertaModel.from_pretrained('roberta-base')\n","\n","def get_embeddings(text):\n","    \"\"\"Compute embeddings for given text using RoBERTa\"\"\"\n","    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    return outputs.last_hidden_state.mean(dim=1).numpy()\n","\n","def compute_skill_similarity(resume_skills, rec_skills):\n","    \"\"\"Compute cosine similarity between resume skills and recommendation skills\"\"\"\n","    embeddings1 = get_embeddings(resume_skills)\n","    embeddings2 = get_embeddings(rec_skills)\n","    return cosine_similarity(embeddings1, embeddings2)[0][0]\n","\n","def process_skills(skills):\n","    \"\"\"Process skills to ensure they are in the correct format\"\"\"\n","    if pd.notna(skills) and isinstance(skills, list) and skills:\n","        return skills[0]\n","    return \"\"\n","\n","def calculate_average_skill_similarity(df_resume, df_lor):\n","    \"\"\"Calculate average skill similarity scores for each resume\"\"\"\n","    average_scores = []\n","\n","    for _, resume_row in df_resume.iterrows():\n","        resume_skills = process_skills(resume_row['Skills'])\n","        if not resume_skills:\n","            average_scores.append(0.0)\n","            continue\n","\n","        recommendations = df_lor[df_lor['Recommendee_ID'] == resume_row['Recommendee_ID']]\n","        similarity_scores = []\n","\n","        for _, rec_row in recommendations.iterrows():\n","            rec_skills = process_skills(rec_row['Skills'])\n","            if rec_skills:\n","                similarity = compute_skill_similarity(resume_skills, rec_skills)\n","                similarity_scores.append(similarity)\n","\n","        avg_score = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.0\n","        average_scores.append(avg_score)\n","\n","    return average_scores\n","\n","def visualize_score_distribution(scores):\n","    \"\"\"Visualize the distribution of skill similarity scores\"\"\"\n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(scores, kde=True)\n","    plt.title('Distribution of Average Skill Similarity Scores')\n","    plt.xlabel('Average Skill Similarity Score')\n","    plt.ylabel('Frequency')\n","    plt.savefig('skill_similarity_distribution.png')\n","    plt.close()\n","\n","def main():\n","    # Load processed data from Section 1\n","    df_resume = pd.read_csv('Files/processed_resume_skills.csv')\n","    df_lor = pd.read_csv('Files/processed_lor_skills.csv')\n","\n","    print(\"Computing skill similarity scores...\")\n","    average_scores = calculate_average_skill_similarity(df_resume, df_lor)\n","\n","    # Add average similarity scores to df_resume\n","    df_resume['Average_Skill_Similarity'] = average_scores\n","\n","    # Combine with existing scores (assuming 'Overall_Score' exists from previous analysis)\n","    if 'Overall_Score' in df_resume.columns:\n","        df_resume['Final_Score'] = 0.7 * df_resume['Overall_Score'] + 0.3 * df_resume['Average_Skill_Similarity']\n","    else:\n","        df_resume['Final_Score'] = df_resume['Average_Skill_Similarity']\n","\n","    # Normalize Final_Score to 0-100 range\n","    df_resume['Final_Score'] = (df_resume['Final_Score'] - df_resume['Final_Score'].min()) / (df_resume['Final_Score'].max() - df_resume['Final_Score'].min()) * 100\n","\n","    print(\"Visualizing score distribution...\")\n","    visualize_score_distribution(df_resume['Final_Score'])\n","\n","    # Save the final results\n","    df_resume.to_csv('Files/final_resume_scores.csv', index=False)\n","    print(\"Final scores saved to 'final_resume_scores.csv'\")\n","\n","    # Display top 10 candidates\n","    top_candidates = df_resume.nlargest(10, 'Final_Score')\n","    print(\"\\nTop 10 Candidates:\")\n","    print(top_candidates[['Recommendee_ID', 'Final_Score']])\n","\n","    # Calculate and display summary statistics\n","    summary_stats = df_resume['Final_Score'].describe()\n","    print(\"\\nSummary Statistics:\")\n","    print(summary_stats)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load data from each level\n","df_level1 = pd.read_csv('innov8/final_ranked_resumes.csv')\n","df_level2 = pd.read_csv('innov8/final_credit_scores.csv')\n","df_level3 = pd.read_csv('innov8/df_resume_sorted.csv')\n","\n","\n","# Normalize scores to 0-100 range\n","def normalize_score(series):\n","    return 100 * (series - series.min()) / (series.max() - series.min())\n","\n","df_combined = pd.DataFrame()\n","\n","# df_combined['ID'] = \n","df_combined['Normalized_Overall_Score'] = normalize_score(df_level1['Overall_Score'])\n","df_combined['Normalized_CreditScore'] = (df_level2['CreditScore'])\n","df_combined['Normalized_Skill_Similarity'] = normalize_score(df_level3['Average_Skill_Similarity'])\n","\n","# Calculate final weighted score\n","df_combined['Final_CV_Score'] = (\n","    0.60 * df_combined['Normalized_Overall_Score'] +\n","    0.25 * df_combined['Normalized_CreditScore'] +\n","    0.15 * df_combined['Normalized_Skill_Similarity']\n",")\n","\n","# Visualize distributions\n","plt.figure(figsize=(15, 10))\n","\n","plt.subplot(2, 2, 1)\n","sns.histplot(df_combined['Normalized_Overall_Score'], kde=True)\n","plt.title('Level 1: Initial Resume Screening Score')\n","\n","plt.subplot(2, 2, 2)\n","sns.histplot(df_combined['Normalized_CreditScore'], kde=True)\n","plt.title('Level 2: Network Analysis Score')\n","\n","plt.subplot(2, 2, 1)\n","sns.histplot(df_combined['Normalized_Skill_Similarity'], kde=True)\n","plt.title('Level 3: Skill Similarity Score')\n","\n","plt.subplot(2, 2, 2)\n","sns.histplot(df_combined['Final_CV_Score'], kde=True)\n","plt.title('Final Weighted CV Score')\n","\n","plt.tight_layout()\n","plt.savefig('score_distributions.png')\n","\n","\n","# Save final results\n","df_combined.to_csv('Files/final_cv_scores.csv', index=False)\n","\n","# Print summary statistics\n","print(\"\\nSummary Statistics of Final CV Score:\")\n","print(df_combined['Final_CV_Score'].describe())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"ats","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"}},"nbformat":4,"nbformat_minor":2}
